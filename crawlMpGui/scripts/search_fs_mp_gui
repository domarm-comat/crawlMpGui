#!/usr/bin/env python3
import argparse
import multiprocessing
import os
import signal
import sys

from PyQt6.QtWidgets import QApplication
from crawlMp.crawlMp import CrawlMp
from crawlMp.crawlers.crawler_fs import CrawlerSearchFs
from crawlMp.enums import Mode

from crawlMpGui import __version__
from crawlMpGui.widgets.results_widget import ResultsWidget

description = [
    f"crawlMp v{__version__}",
    "MultiProcess recursive file search.",
    "Default starting location is Current Working Directory.",
    "",
    "Usage examples:",
    "  Search for all .zip files:",
    "  search_fs_mp \\\\.zip$",
    "",
    "  Get all .zip files in different directories:",
    "  search_fs_mp \\\\.zip$ -l /home /usr/share",
    "",
    "  Show search summary:",
    "  search_fs_mp \\\\.zip$ -l /home /usr/share -os",
    "",
]

arguments = sys.argv
parser = argparse.ArgumentParser(
    formatter_class=argparse.RawTextHelpFormatter,
    description="\r\n".join(description)
)
parser.add_argument("pattern", default=".", type=str, help="RegExp filename pattern to search for")
parser.add_argument("-l", "--links", type=str, nargs="+",
                    help="Entry point(s) to start search from.", default=[os.getcwd()])
parser.add_argument("-np", "--processes", default=multiprocessing.cpu_count(), type=int,
                    help="Number of processes used, minimum is 1")
parser.add_argument("-v", "--version", help="Show crawlMp version", action='store_true')
parser.add_argument("-bs", "--buffer_size", default=96, type=int, help="Links buffer size")
args = parser.parse_args()

if args.version:
    print(f"crawlMp v{__version__}")

app = QApplication(sys.argv)
w = ResultsWidget()
w.show()

manager = CrawlMp(CrawlerSearchFs, links=args.links, num_proc=args.processes, buffer_size=args.buffer_size,
                  pattern=args.pattern, mode=Mode.EXTENDED)
signal.signal(signal.SIGINT, lambda sig, frame: manager.stop())
manager.start(lambda m: w.sig_update_results.emit(m.results))

app.exec()
